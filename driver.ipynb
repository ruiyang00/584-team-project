{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0bb1017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from copy import copy, deepcopy\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "deluxe-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_option(\"display.max_columns\")\n",
    "df2 = pd.read_csv(\"cup98lrn.txt\", sep=',', error_bad_lines = False, low_memory = False, skip_blank_lines = True)\n",
    "df1 = pd.read_csv(\"cup98lrn.txt\", sep=',', error_bad_lines = False, low_memory = False, skip_blank_lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression(data):\n",
    "    # train test split\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    # get targets out\n",
    "    train_B = train[\"TARGET_B\"]\n",
    "    train_D = train[\"TARGET_D\"]\n",
    "    test_B = test[\"TARGET_B\"]\n",
    "    test_D = test[\"TARGET_D\"]\n",
    "    train.drop(columns = [\"TARGET_D\",\"TARGET_B\"], inplace = True)\n",
    "    \n",
    "    # we need to resample the train data to balance it out\n",
    "    sampler = RandomOverSampler(random_state=50)\n",
    "    x_res, y_res = over_sampler.fit_resample(train, train_B)\n",
    "    print(f\"Training target statistics: {Counter(y_res)}\")\n",
    "    print(f\"Testing target statistics: {Counter(y_test)}\")\n",
    "    \n",
    "    # train the model\n",
    "    clf = LogisticRegression(max_iter = 100, solver = \"liblinear\", verbose = 1)\n",
    "    clf = clf.fit(x_res, y_res)\n",
    "    \n",
    "    # test on the test set\n",
    "    y_pred = clf.predict(test)\n",
    "    \n",
    "    return get_acc(y_pred, test_B, test_D, 0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(y_pred, y_actual, y_donate, mail_cost):\n",
    "    df = pd.concat([y_pred, y_actual, y_donate], axis = 1)\n",
    "    df.columns = [\"y_pred\", \"y_actual\", \"y_donate\"]\n",
    "    \n",
    "    # get false positive rate\n",
    "    fp_rate = df[(df['y_pred'] == 1) & (df['y_actual'] == 0)].shape[0]\n",
    "    # get false negative rate\n",
    "    fn_rate = df[(df['y_pred'] == 0) & (df['y_actual'] == 1)].shape[0]\n",
    "    # get total profit \n",
    "    profit = df[(df['y_pred'] == 1) & (df['y_actual'] == 1)][\"y_donate\"].sum() - df[(df['y_pred'] == 1)].shape[0]*mail_cost\n",
    "    \n",
    "    return fp_rate, fn_rate, profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70cc93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data preprocessing tasks include the following(https://kdd.org/cupfiles/KDDCupData/1998/cup98doc.txt)\n",
    "\n",
    "1. Noisy Data\n",
    "Some of the fields in the analysis file may contain data entry and/or\n",
    "formatting errors. You are expected to clean these fields (without\n",
    "excluding the records.)\n",
    "Rui: I think dataframe already takes care of this part. Dont think there is any data record as formatting issue\n",
    "\n",
    "2. Records and Fields with Missing and Sparse Data\n",
    "    2.1:Drop the attribute when it missing more than 99.5% \n",
    "    2.2:replace the missing record with the (mean,mode,avg, most common type, etc) \n",
    "            value of the avg. value of attribute\n",
    "3.Fields Containing Constants\n",
    "4.Time Frame and Date Fields     \n",
    "'''\n",
    "\n",
    "def trim_data(df):\n",
    "    \n",
    "    #I think 1 is already done by dataframe\n",
    "    \n",
    "    \n",
    "    \n",
    "    #there are a lot whitesapces in features. I replace whitespaces with nan first\n",
    "    # for the convenience of data processing\n",
    "    df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "    \n",
    "    num_cols = len(df.columns)\n",
    "    num_rows = len(df)\n",
    "    ####Fields Containing Constants################\n",
    "    df.dropna(axis = 0, thresh= 2, inplace=True)\n",
    "    \n",
    "    ####dealing with missing values#################\n",
    "        \n",
    "    #1. drop the attribute if missing values >= 99.5%\n",
    "    #calculating the dropping_treshold \n",
    "    perc = 99.5\n",
    "    min_count =  int(((100-perc)/100)*num_rows+ 1)\n",
    "    df.dropna(axis = 1, thresh=min_count, inplace=True)\n",
    "    \n",
    "    #2. if features contains NAN < 99.5% we need to replace NAN with the most frequent value\n",
    "    #this line does replace differnet attribute types(Number, char, boolean, etc)  with the most frequent\n",
    "    # value\n",
    "    df.fillna(df.mode().iloc[0], inplace=True)\n",
    "    \n",
    "    \n",
    "    ####dealing with sparse values#################\n",
    "    \n",
    "    \n",
    "    ####Time Frame and Date Fields#########\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
