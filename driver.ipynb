{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bb1017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import copy, deepcopy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "import collections\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, SelectFpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "reverse-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_B = [ # TARGET_B\n",
    "    DecisionTreeClassifier(max_depth = 20), # tried 40, 60, 80, same\n",
    "    LogisticRegression(max_iter = 100, solver = \"liblinear\"), # tried 200, 400, 800, same\n",
    "    AdaBoostClassifier(n_estimators=100, learning_rate=0.5),\n",
    "    #MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, learning_rate_init=0.005),\n",
    "    # # RandomForestClassifier(), # give very low profit\n",
    "    # SVC(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "frank-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_D = [\n",
    "    # LinearRegression(),\n",
    "    #MLPRegressor(hidden_layer_sizes=(50,25), max_iter=500, learning_rate_init=0.005),\n",
    "    #SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "continuous-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(data, balanced_sampling=True):\n",
    "    result = {\n",
    "        m: {'acc': [], 'fp': [], 'fn': [], 'profit': []} for m in models_B + models_D\n",
    "    }\n",
    "    # train, test = train_test_split(data, test_size=0.3)\n",
    "    kf = KFold(n_splits=5)\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        train = deepcopy(data.iloc[train_index])\n",
    "        test = deepcopy(data.iloc[test_index])\n",
    "        # get targets out\n",
    "        train_B = train.loc[:,\"TARGET_B\"]\n",
    "        train_D = train.loc[:,\"TARGET_D\"]\n",
    "        test_B = test.loc[:,\"TARGET_B\"]\n",
    "        test_D = test.loc[:,\"TARGET_D\"]\n",
    "        train.drop(columns = [\"TARGET_D\", \"TARGET_B\"], inplace = True)\n",
    "        test.drop(columns = [\"TARGET_D\", \"TARGET_B\"], inplace = True)\n",
    "        # we need to resample the train data to balance it out\n",
    "        if balanced_sampling:\n",
    "            x_res_B, y_res_B = RandomOverSampler(random_state=10000).fit_resample(train, train_B)\n",
    "            train1 = deepcopy(train)\n",
    "            train1.loc[:,'TARGET_D'] = train_D\n",
    "            x_res_D, y_res_D = train, train_D # smogn.smoter(train1, 'TARGET_D', rel_method='manual') not working\n",
    "        else:\n",
    "            x_res_B, y_res_B = train, train_B\n",
    "            x_res_D, y_res_D = train, train_D\n",
    "        # print(\"oversampled to \"+str(x_res_B.shape[0])+\" data points for classification.\")\n",
    "        # run the model\n",
    "        for clf in models_B + models_D:\n",
    "            acc, fp, fn, profit = run_classifier(clf, x_res_B, y_res_B, test, test_B, test_D, regression=clf in models_D)\n",
    "            result[clf]['acc'].append(acc)\n",
    "            result[clf]['fp'].append(fp)\n",
    "            result[clf]['fn'].append(fn)\n",
    "            result[clf]['profit'].append(profit)\n",
    "    for m in result:\n",
    "        for metric in ['acc', 'fp', 'fn', 'profit']:\n",
    "            print(m, np.mean(result[m][metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "wrapped-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_models(train_data, val_data, balanced_sampling=True):\n",
    "    result = {\n",
    "        m: {'acc': None, 'fp': None, 'fn': None, 'profit': None} for m in models_B + models_D\n",
    "    }\n",
    "    train, test = train_data, val_data\n",
    "    # get targets out\n",
    "    train_B = train[\"TARGET_B\"]\n",
    "    train_D = train[\"TARGET_D\"]\n",
    "    test_B = test[\"TARGET_B\"]\n",
    "    test_D = test[\"TARGET_D\"]\n",
    "    train.drop(columns = [\"TARGET_D\", \"TARGET_B\"], inplace = True)\n",
    "    test.drop(columns = [\"TARGET_D\", \"TARGET_B\"], inplace = True)\n",
    "    # we need to resample the train data to balance it out\n",
    "    if balanced_sampling:\n",
    "        ## over sample\n",
    "        #x_res_B, y_res_B = RandomOverSampler(random_state=10000).fit_resample(train, train_B)\n",
    "        ## down sample\n",
    "        x_res_B, y_res_B = RandomDownSampler(random_state=10000).fit_resample(train, train_B)\n",
    "        ## SMOTE\n",
    "        #x_res_B, y_res_B = SMOTE(random_state=10000).fit_resample(train, train_B)\n",
    "        \n",
    "        train1 = deepcopy(train)\n",
    "        train1['TARGET_D'] = train_D\n",
    "        x_res_D, y_res_D = train, train_D # smogn.smoter(train1, 'TARGET_D', rel_method='manual') not working\n",
    "    else:\n",
    "        x_res_B, y_res_B = train, train_B\n",
    "        x_res_D, y_res_D = train, train_D\n",
    "    # print(\"oversampled to \"+str(x_res_B.shape[0])+\" data points for classification.\")\n",
    "    # run the model\n",
    "    for clf in models_B + models_D:\n",
    "        acc, fp, fn, profit = run_classifier(clf, x_res_B, y_res_B, test, test_B, test_D, regression=clf in models_D)\n",
    "        result[clf]['acc'] = acc\n",
    "        result[clf]['fp'] = fp\n",
    "        result[clf]['fn'] = fn\n",
    "        result[clf]['profit'] = profit\n",
    "    for m in result:\n",
    "        for metric in ['acc', 'fp', 'fn', 'profit']:\n",
    "            print(m, result[m][metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "opposed-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(clf, x_res, y_res, test, test_B, test_D, regression=False):\n",
    "    print(clf)\n",
    "    clf = clf.fit(x_res, y_res)\n",
    "    y_pred = clf.predict(test)\n",
    "    if regression:\n",
    "        acc, fp, fn, profit = get_acc_regressor(y_pred, test_B, test_D, 0.68)\n",
    "    else:\n",
    "        acc, fp, fn, profit = get_acc(y_pred, test_B, test_D, 0.68)\n",
    "    #print(\"accuracy = \"+str(acc))\n",
    "    #print(\"false positive rate = \"+str(fp))\n",
    "    #print(\"false negative rate = \"+str(fn))    \n",
    "    #print(\"profit = \"+str(profit))\n",
    "    return acc, fp, fn, profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "moderate-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(y_pred, y_actual, y_donate, mail_cost):\n",
    "    df = pd.concat([pd.Series(y_pred, index=y_actual.index), pd.Series(y_actual), pd.Series(y_donate)], axis = 1)\n",
    "    df.columns = [\"y_pred\", \"y_actual\", \"y_donate\"]\n",
    "    \n",
    "    #get accuracy\n",
    "    accuracy = df[(df['y_pred'] == df['y_actual'])].shape[0] / y_actual.shape[0]\n",
    "    # get false positive rate\n",
    "    fp_rate = df[(df['y_pred'] == 1) & (df['y_actual'] == 0)].shape[0] / y_actual.shape[0]\n",
    "    # get false negative rate\n",
    "    fn_rate = df[(df['y_pred'] == 0) & (df['y_actual'] == 1)].shape[0] / y_actual.shape[0]\n",
    "    # get total profit \n",
    "    profit = df[(df['y_pred'] == 1) & (df['y_actual'] == 1)][\"y_donate\"].sum() - df[(df['y_pred'] == 1)].shape[0]*mail_cost\n",
    "    \n",
    "    return accuracy, fp_rate, fn_rate, profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "continent-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_regressor(y_pred, y_actual, y_donate, mail_cost):\n",
    "    df = pd.concat([pd.Series(y_pred, index=y_actual.index), pd.Series(y_actual), pd.Series(y_donate)], axis = 1, ignore_index=True)\n",
    "    df.columns = [\"y_pred\", \"y_actual\", \"y_donate\"]\n",
    "    \n",
    "    #get accuracy\n",
    "    accuracy = df[((df['y_pred'] > mail_cost) & (df['y_actual'])) | ((df['y_pred'] <= mail_cost) & (df['y_actual'] == 0))].shape[0] / y_actual.shape[0]\n",
    "    # get false positive rate\n",
    "    fp_rate = df[(df['y_pred'] > mail_cost) & (df['y_actual'] == 0)].shape[0] / y_actual.shape[0]\n",
    "    # get false negative rate\n",
    "    fn_rate = df[(df['y_pred'] <= mail_cost) & (df['y_actual'] == 1)].shape[0] / y_actual.shape[0]\n",
    "    # get total profit \n",
    "    profit = df[(df['y_pred'] > mail_cost) & (df['y_actual'] == 1)]['y_donate'].sum() - df[(df['y_pred'] > mail_cost)].shape[0]*mail_cost\n",
    "    \n",
    "    return accuracy, fp_rate, fn_rate, profit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b70cc93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(df):     \n",
    "    # need to be done first\n",
    "    for key in ['NOEXCH', 'RECINHSE', 'RECP3', 'RECPGVG', 'RECSWEEP', 'MAILCODE', 'PEPSTRFL']:\n",
    "        df.loc[df[key].isin([\"0\", \"1\", \" \", 0, 1]), key]= 0\n",
    "        df.loc[df[key].isin([\"X\"]), key] = 1\n",
    "    \n",
    "    df.loc[:,'ZIP'] = df.loc[:,'ZIP'].astype(str)\n",
    "    df.loc[:,'ZIP'] = df.loc[:,'ZIP'].str.slice(0,5)\n",
    "    \n",
    "    \n",
    "    ''' General:\n",
    "        replacing any value with period or/and whitespace\n",
    "    '''\n",
    "    \n",
    "    #whitesapce \\s\n",
    "    \n",
    "    df.drop(labels=['CONTROLN', 'ZIP'], axis = 1, inplace=True)\n",
    "    df.select_dtypes(include=['object']).replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "    df.select_dtypes(include=np.number).replace(r'^\\s.*$', np.nan, regex=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    ####dealing with missing features#################   \n",
    "    #1. drop the attribute if missing values >= 99.5%\n",
    "    #calculating the dropping_treshold \n",
    "    num_rows = len(df)\n",
    "    perc = 99.5\n",
    "    min_count =  int(((100-perc)/100)*num_rows+ 1)\n",
    "    df.dropna(axis = 1, thresh=min_count)\n",
    "    \n",
    "    #2. if features contains NAN < 99.5% we need to replace NAN with the most frequent value\n",
    "    #this line does replace differnet attribute types(Number, char, boolean, etc)  with the most frequent\n",
    "    # value\n",
    "    df.fillna(df.mode().iloc[0], inplace=True)\n",
    "    \n",
    "    \n",
    "    ### categorical data ##########\n",
    "    for key in df.select_dtypes(include=['object']).columns:\n",
    "        mapping = {k: v+1 for v, k in enumerate(sorted([str(a) for a  in df[key].unique()]))} \n",
    "        df[key].replace(mapping, inplace=True)\n",
    "    ####Time Frame and Date Fields#########\n",
    "    end_date = 9706\n",
    "    for time_key in ['MAXADATE', 'MINRDATE', 'MAXRDATE', 'LASTDATE', 'FISTDATE', 'NEXTDATE', 'ODATEDW']: \n",
    "        end_date = pd.to_datetime(end_date, format='%y%m', exact=True)\n",
    "        df.loc[df[time_key] == 0, time_key] = df[time_key].mode()\n",
    "        start_date = temp_date_attr = pd.to_datetime(df[time_key], format='%y%m', exact=True)\n",
    "        df.loc[:,time_key] = (end_date - start_date).dt.days/30\n",
    "    df.fillna(df.mode().iloc[0], inplace=True)\n",
    "    ####Fields Containing Constants################\n",
    "    df = df.loc[:, (df != df.iloc[0]).any()] \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "finite-legislature",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wayne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\frame.py:4383: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n",
      "c:\\users\\wayne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\frame.py:4167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "## cross-validation on training set\n",
    "\n",
    "df = pd.read_csv(\"cup98lrn.txt\", sep=',', error_bad_lines = False, low_memory = False, skip_blank_lines = True)\n",
    "data_trimmed = preprocessing_data(df)\n",
    "# data_trimmed.to_csv('data_trimmed.csv', index = False)\n",
    "# data_trimmed = pd.read_csv(\"data_trimmed.csv\", sep=',', error_bad_lines = False, low_memory = False, skip_blank_lines = True)\n",
    "targets = deepcopy(data_trimmed[['TARGET_D', 'TARGET_B']])\n",
    "data_trimmed.drop(columns = ['TARGET_D', 'TARGET_B'], inplace = True)\n",
    "data_trimmed = (data_trimmed - data_trimmed.min())/(data_trimmed.max() - data_trimmed.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "canadian-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = SelectKBest(score_func=f_classif, k=200).fit_transform(data_trimmed, targets[\"TARGET_B\"]) # f_regression for \"TARGET_D\"\n",
    "data_selected = pd.DataFrame(features)\n",
    "data = pd.concat([data_selected, targets], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-wales",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=20)\n",
      "LogisticRegression(solver='liblinear')\n",
      "AdaBoostClassifier(learning_rate=0.5, n_estimators=100)\n",
      "DecisionTreeClassifier(max_depth=20)\n",
      "LogisticRegression(solver='liblinear')\n",
      "AdaBoostClassifier(learning_rate=0.5, n_estimators=100)\n",
      "DecisionTreeClassifier(max_depth=20)\n",
      "LogisticRegression(solver='liblinear')\n",
      "AdaBoostClassifier(learning_rate=0.5, n_estimators=100)\n",
      "DecisionTreeClassifier(max_depth=20)\n",
      "LogisticRegression(solver='liblinear')\n",
      "AdaBoostClassifier(learning_rate=0.5, n_estimators=100)\n",
      "DecisionTreeClassifier(max_depth=20)\n",
      "LogisticRegression(solver='liblinear')\n",
      "AdaBoostClassifier(learning_rate=0.5, n_estimators=100)\n"
     ]
    }
   ],
   "source": [
    "compare_models(data, balanced_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
